The sequential version of {\tt PSTA} takes $O(N+\sqrt{2^{w}}poly(w))$,
the same complexity reported in
\cite{Navarro:2014:FFS:2620785.2601073}. The amount of work of {\tt
PSTA}, $T_1$, is also $O(N+\sqrt{2^{w}}poly(w))$, dominated mainly by
Part I of Algorithm (\ref{algo:PSTA1}). The complexity of {\tt PSTA},
considering $p$ processors, is $T_p =
O(\frac{N}{p}+p+\frac{\sqrt{2^{w}}poly(w)}{p})$. Now, the complexity
with enough amount of processors, $T_\infty$, is $O(N)$. The speedup
of our algorithm is $T_1/T_P$ =
$O(\frac{N+\sqrt{2^{w}}poly(w)}{\frac{N+\sqrt{2^{w}}poly(w)}{p}+p})$
\Leo{should we put these long formulas in a new line?}. As we can see,
when $p$ is small, the speedup tends to $O(p)$, which meets our
assumption of $p\ll N$. However, when $p$ tends to $N$, the speedup
tends to decrease. This last observation is more evident in the
analysis of parallelism, $T_1/T_{\infty}$ =
$\frac{N+\sqrt{2^{w}}poly(w)}{N}$, which tends to $1$. The assumption
$p\ll N$ is realistic, especially considering current SMP-like
systems, where the amount of available processing units, even though
they have increased in the past few years, is much less than the
input, $N$ in this case. Moreover, trees have become extremely large,
several-million-node trees, such as suffix trees of text collections
and XML collections are not difficult to find. \Leo{which are how big
exactly, for example?}.

An important advantage of our algorithm is its working space. The {\tt
PSTA} algorithm does not need any extra memory related to the usage of
threads. Indeed, it just needs a working space proportional to the
input size and the space needed to schedule the threads. A
work-stealing scheduler, as that used by the DYM model, exhibit at
most a linear expansion space, that is, $O(S_1p)$, where $S_1$ is the
minimum amount of space used by the scheduler for any execution of a
multithreaded computation using one processor. This upper bound is
optimal to within a constant factor
\cite{Blumofe:1999:SMC:324133.324234}. In summary, the working space
needed by our algorithm is $O(N+S_1p)$. Thus, considering the
assumption $p\ll N$, the working space is dominated by the input
size. The scheduling space is negligible.

The algorithm reaches its optimal behavior when the work of building
each subtree in parallel (lines 2 to 18 of Algorithm \ref{algo:PSTA2})
is the same as the work needed to create the top part of the {\tt
RMMT} (lines 19 to 32 of Algorithm \ref{algo:PSTA2}). This point is
reached when the $p\log_{k}p=N/s$. 
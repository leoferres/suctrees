In the DyM model \cite[Chapter~27]{Cormen2009}, a {\em multithreaded
computation} is modelled as a directed acyclic graph $G=(V,E)$
whose vertices are instructions and where $(u,v) \in E$ if
$u$ must be executed before $v$.  The time $T_p$ needed to execute the
computation on $p$ cores depends on two parameters of the computation:
its {\em work} $T_1$ and its {\em span} $T_\infty$.  The work is the
running time on a single core, equal to the number of nodes
(i.e., instructions) in $G$, assuming each instruction takes constant
time.  Since $p$ cores can execute only $p$ instructions at a time, we
have $T_p = \Omega(T_1/p)$.  The span is the length of the longest
path in~$G$.  Since the instructions on this path need to be executed
in order, we also have $T_p = \Omega(T_\infty)$.
Together, these two lower bounds give $T_p = \Omega(T_\infty +
T_1/p)$.
Work-stealing schedulers match the optimal bound to within
a factor of 2~\cite{Blumofe:1999:SMC:324133.324234}.  The degree to
which an algorithm can take advantage of the presence of $p > 1$ cores
is captured by its {\em speed-up} $T_1 / T_p$ and its {\em
parallelism} $T_1 / T_\infty$.  In the absence of cache effects, the
best possible speed-up is $p$, known as {\em linear speed-up}.
Parallelism provides an upper bound on the achievable speed-up.

To describe parallel algorithms in the DYM model, we augment
sequential pseudocode with three keywords.  The {\bf spawn} keyword,
followed by a procedure call, indicates that the procedure should run
in its own thread and {\em may} thus be executed in parallel to the
thread that spawned it.  The {\bf sync} keyword
indicates that the current thread must wait for the termination of all
threads it has spawned.  It thus provides a
simple barrier-style synchronization mechanism.  Finally, {\bf parfor}
is ``syntactic sugar'' for {\bf spawn}ing one thread per iteration in
a for loop, thereby allowing these iterations to run in parallel,
followed by a {\bf sync} operation that waits for all iterations to
complete.  In practice, the overhead is logarithmic in the number of
iterations.  When a procedure exits, it implicitly performs a {\bf sync} to
ensure all threads it spawned finish first.
